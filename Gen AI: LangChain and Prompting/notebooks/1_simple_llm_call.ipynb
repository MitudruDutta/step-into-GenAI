{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3375081157f73bc",
   "metadata": {},
   "source": [
    "# üöÄ Simple LLM Calls with LangChain\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to make your first LLM (Large Language Model) calls using **LangChain**. You'll learn to interact with both cloud-hosted and locally-running models.\n",
    "\n",
    "## What You'll Learn\n",
    "- Setting up API keys securely with environment variables\n",
    "- Making LLM calls using **Groq** (cloud-hosted)\n",
    "- Making LLM calls using **Ollama** (local models)\n",
    "- Configuring model parameters (temperature, max_tokens)\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install langchain langchain-groq langchain-ollama python-dotenv\n",
    "```\n",
    "\n",
    "### Resources\n",
    "- [LangChain Installation Guide](https://python.langchain.com/docs/how_to/installation/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907dad2e",
   "metadata": {},
   "source": [
    "## 1. Load API Key\n",
    "\n",
    "API keys should **never** be hardcoded in your code. We use `python-dotenv` to load them from a `.env` file.\n",
    "\n",
    "**Create a `.env` file** in your project root:\n",
    "```\n",
    "GROQ_API_KEY=your_groq_api_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066dd132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import  load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c688125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\n",
    "        \"Missing GROQ_API_KEY. Add it to your .env or export GROQ_API_KEY before running.\"\n",
    "    )\n",
    "print(\"GROQ_API_KEY loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88cc6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Cloud LLM Calls with Groq\n",
    "\n",
    "**Groq** provides fast inference for popular open-source models. It's a great option for:\n",
    "- Rapid prototyping\n",
    "- Production applications requiring low latency\n",
    "- Accessing models like Llama, Mixtral, and more\n",
    "\n",
    "### Basic Call\n",
    "Simply specify the model name and invoke with your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4f667f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe810bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first person to set foot on the Moon was **Neil\\u202fArmstrong**. \\n\\n- **Mission:** Apollo\\u202f11 (NASA)  \\n- **Date of lunar landing:**\\u202f20\\u202fJuly\\u202f1969 (UTC)  \\n- **Landing site:**\\u202fSea of Tranquility (Mare Tranquillitatis)  \\n- **Famous quote:** ‚ÄúThat‚Äôs one small step for [a] man, one giant leap for mankind.‚Äù\\n\\nNeil\\u202fArmstrong stepped onto the lunar surface at about\\u202f02:56\\u202fUTC, followed shortly thereafter by **Buzz\\u202fAldrin**, who became the second person to walk on the Moon. The third astronaut on the mission, **Michael\\u202fCollins**, remained in lunar orbit aboard the Command Module\\u202f*Columbia*.', additional_kwargs={'reasoning_content': 'The user says \"The person to go to moon was\". Possibly they want to know who was the first person to go to the Moon? The phrase incomplete. Likely they ask \"The person to go to moon was\". So answer: Neil Armstrong was the first person to set foot on the Moon (Apollo 11, July 20, 1969). Could also mention Buzz Aldrin second. Provide context.'}, response_metadata={'token_usage': {'completion_tokens': 251, 'prompt_tokens': 78, 'total_tokens': 329, 'completion_time': 0.528908029, 'completion_tokens_details': {'reasoning_tokens': 85}, 'prompt_time': 0.003411148, 'prompt_tokens_details': None, 'queue_time': 0.055155862, 'total_time': 0.532319177}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_a09bde29de', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b8908-7366-7e63-a0f3-d31a7fbce294-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 78, 'output_tokens': 251, 'total_tokens': 329, 'output_token_details': {'reasoning': 85}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"openai/gpt-oss-120b\"\n",
    ")\n",
    "llm.invoke(\"The person to go to moon was\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78e1abc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first person to walk on the moon was Neil Armstrong. He stepped out', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 42, 'total_tokens': 57, 'completion_time': 0.023161708, 'completion_tokens_details': None, 'prompt_time': 0.001959152, 'prompt_tokens_details': None, 'queue_time': 0.063660648, 'total_time': 0.02512086}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'length', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b8908-8aa4-7e31-823f-f9e6575be33e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 42, 'output_tokens': 15, 'total_tokens': 57})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.6,\n",
    "    max_tokens=15\n",
    ")\n",
    "llm.invoke(\"The person to go to moon was\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1862135",
   "metadata": {},
   "source": [
    "### Configuring Model Parameters\n",
    "\n",
    "Key parameters to control LLM behavior:\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `temperature` | Randomness/creativity (0 = deterministic, 1 = creative) | 0.0 - 1.0 |\n",
    "| `max_tokens` | Maximum length of response | 50 - 4096 |\n",
    "| `top_p` | Nucleus sampling (alternative to temperature) | 0.0 - 1.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fdae1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Local LLM Calls with Ollama\n",
    "\n",
    "**Ollama** lets you run LLMs locally on your machine. Benefits:\n",
    "- **Privacy**: Data never leaves your machine\n",
    "- **No API costs**: Free to use (just compute costs)\n",
    "- **Offline**: Works without internet\n",
    "\n",
    "### Prerequisites for Ollama\n",
    "1. Install Ollama: https://ollama.ai\n",
    "2. Pull a model: `ollama pull mistral`\n",
    "3. Start Ollama server: `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd7169be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The first person to set foot on the Moon was **Neil Armstrong**,', additional_kwargs={}, response_metadata={'model': 'ministral-3:8b', 'created_at': '2026-01-04T12:45:35.915515743Z', 'done': True, 'done_reason': 'length', 'total_duration': 1632950612, 'load_duration': 88042417, 'prompt_eval_count': 560, 'prompt_eval_duration': 675068875, 'eval_count': 15, 'eval_duration': 845538128, 'logprobs': None, 'model_name': 'ministral-3:8b', 'model_provider': 'ollama'}, id='lc_run--019b890a-a2c9-7ab0-9e05-8d4aeee038f8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 560, 'output_tokens': 15, 'total_tokens': 575})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"ministral-3:8b\",temperature=0, num_predict=15)\n",
    "llm.invoke(\"The person to go to moon was\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d75d55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "| Approach | Provider | Use Case |\n",
    "|----------|----------|----------|\n",
    "| **Cloud** | Groq | Fast inference, no local GPU needed |\n",
    "| **Local** | Ollama | Privacy, offline use, no API costs |\n",
    "\n",
    "### Key Takeaways\n",
    "1. Use environment variables for API keys\n",
    "2. `temperature` controls creativity (0 = factual, 1 = creative)\n",
    "3. `max_tokens` limits response length\n",
    "4. LangChain provides a unified interface for different LLM providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
