{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00607318",
   "metadata": {},
   "source": [
    "# üìù Prompt Templates and Chains\n",
    "\n",
    "## Overview\n",
    "This notebook introduces two core LangChain concepts: **Prompt Templates** and **Chains**. These are essential building blocks for creating reusable, maintainable AI applications.\n",
    "\n",
    "## What You'll Learn\n",
    "- Creating reusable prompt templates with variables\n",
    "- Building chains to connect components\n",
    "- Using the pipe (`|`) operator for chain composition\n",
    "- Understanding LangChain's LCEL (LangChain Expression Language)\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install langchain langchain-groq python-dotenv\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e53be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5ce87",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import the required modules and load environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795b8cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9297652a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Write two line poem on {topic}')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write two line poem on {topic}\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d610299b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Prompt Templates\n",
    "\n",
    "### What is a Prompt Template?\n",
    "A **Prompt Template** is a reusable structure for prompts with placeholders (variables) that get filled in at runtime.\n",
    "\n",
    "**Benefits:**\n",
    "- ‚úÖ Reusable across different inputs\n",
    "- ‚úÖ Consistent prompt structure\n",
    "- ‚úÖ Easy to modify and maintain\n",
    "- ‚úÖ Separates prompt logic from code\n",
    "\n",
    "**Syntax:** Use `{variable_name}` for placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b636bee2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='A delicate butterfly dances free, \\nSpreading beauty and wonder, for you and me.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 41, 'total_tokens': 60, 'completion_time': 0.112551087, 'completion_tokens_details': None, 'prompt_time': 0.001170486, 'prompt_tokens_details': None, 'queue_time': 0.053721074, 'total_time': 0.113721573}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b890a-43e8-7d32-9c17-42fefe10ce0b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 41, 'output_tokens': 19, 'total_tokens': 60})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({'topic': 'butterfly'})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670b01d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Chains with LCEL\n",
    "\n",
    "### What is a Chain?\n",
    "A **Chain** connects multiple components together, passing output from one to the next.\n",
    "\n",
    "### LCEL (LangChain Expression Language)\n",
    "The pipe operator `|` creates chains:\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "This reads as: \"Take the prompt ‚Üí send to LLM ‚Üí parse the output\"\n",
    "\n",
    "### How It Works:\n",
    "1. `prompt.invoke({'topic': 'butterfly'})` ‚Üí Creates the full prompt text\n",
    "2. `llm.invoke(prompt_text)` ‚Üí Sends to LLM and gets response\n",
    "3. Chain handles the data flow automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ce96bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A delicate butterfly dances free, \n",
      "Spreading beauty and wonder, for you and me.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f80b68",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Description | Example |\n",
    "|---------|-------------|---------|\n",
    "| **PromptTemplate** | Reusable prompt with variables | `\"Write about {topic}\"` |\n",
    "| **Chain** | Connected components | `prompt \\| llm` |\n",
    "| **LCEL** | Pipe syntax for chains | `\\|` operator |\n",
    "\n",
    "### Chain Flow\n",
    "```\n",
    "Input Dict ‚Üí PromptTemplate ‚Üí LLM ‚Üí Response\n",
    "{'topic': 'AI'}  ‚Üí  \"Write about AI\"  ‚Üí  \"AI is...\"\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "- Add output parsers to structure responses\n",
    "- Create more complex multi-step chains\n",
    "- Use chat message templates for conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc11c81",
   "metadata": {},
   "source": [
    "### Accessing the Response\n",
    "\n",
    "The response object contains:\n",
    "- `content`: The actual text response\n",
    "- `response_metadata`: Model info, token usage, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
