{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Key Parameters in Text Generation\n",
    "\n",
    "This notebook explores the **key parameters** that control how Large Language Models (LLMs) generate text. Understanding these parameters is essential for getting the desired output from generative AI models.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- **Temperature**: Controls randomness and creativity\n",
    "- **Top-p (Nucleus Sampling)**: Probability-based token selection\n",
    "- **Top-k**: Limits token choices to top k candidates\n",
    "- **Output Length**: Controls how much text is generated\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5",
   "metadata": {},
   "source": [
    "## üì¶ Setup\n",
    "\n",
    "First, let's import the necessary libraries. We'll use the `transformers` library from Hugging Face, which provides easy access to pre-trained language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "## ü§ñ Loading the Model\n",
    "\n",
    "We'll use **GPT-2**, a popular open-source language model from OpenAI. While smaller than modern models like GPT-4, it's perfect for demonstrating these concepts.\n",
    "\n",
    "The `pipeline` function creates an easy-to-use interface for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üå°Ô∏è Temperature\n",
    "\n",
    "**Temperature** is one of the most important parameters in text generation. It controls the **randomness** of the model's output.\n",
    "\n",
    "### How Temperature Works\n",
    "\n",
    "When a model generates text, it predicts probabilities for each possible next token (word/subword). Temperature modifies these probabilities:\n",
    "\n",
    "| Temperature | Effect | Use Case |\n",
    "|-------------|--------|----------|\n",
    "| **Low (0.1-0.3)** | More deterministic, focused, repetitive | Factual content, code, precise answers |\n",
    "| **Medium (0.5-0.7)** | Balanced creativity and coherence | General writing, conversations |\n",
    "| **High (0.8-1.5)** | More random, creative, diverse | Creative writing, brainstorming |\n",
    "\n",
    "### Mathematical Intuition\n",
    "\n",
    "Temperature divides the logits (raw model outputs) before applying softmax:\n",
    "- **Low temperature** ‚Üí Sharpens the distribution (high-probability tokens become even more likely)\n",
    "- **High temperature** ‚Üí Flattens the distribution (all tokens become more equally likely)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "### Example: Low Temperature (0.1)\n",
    "\n",
    "With a very low temperature, the model becomes **highly deterministic**. It will almost always choose the most probable next token, leading to predictable (and sometimes repetitive) output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperature = More focused, deterministic output\n",
    "result = generator(\n",
    "    \"Here is a suggestion on my new coffee shop name. The name should be:\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Temperature 0.1 (Low - Deterministic):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9",
   "metadata": {},
   "source": [
    "### Example: High Temperature (1.2)\n",
    "\n",
    "With a higher temperature, the model becomes **more creative and diverse**. It's more willing to choose less probable tokens, leading to more varied (but potentially less coherent) output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab908e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High temperature = More creative, diverse output\n",
    "result = generator(\n",
    "    \"Here is a suggestion on my new coffee shop name. The name should be:\",\n",
    "    temperature=1.2,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Temperature 1.2 (High - Creative):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "### üí° Key Takeaway\n",
    "\n",
    "- **Low temperature** ‚Üí Safe, predictable, may repeat\n",
    "- **High temperature** ‚Üí Creative, diverse, may be incoherent\n",
    "- **Sweet spot** ‚Üí Usually between 0.7-0.9 for most tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1",
   "metadata": {},
   "source": [
    "## üéØ Top-p (Nucleus Sampling)\n",
    "\n",
    "**Top-p** (also called **nucleus sampling**) is an alternative to temperature for controlling randomness. Instead of modifying probabilities, it **limits which tokens can be selected**.\n",
    "\n",
    "### How Top-p Works\n",
    "\n",
    "1. Sort all possible next tokens by probability (highest first)\n",
    "2. Add tokens to the \"nucleus\" until their cumulative probability reaches `p`\n",
    "3. Sample only from tokens in the nucleus\n",
    "\n",
    "| Top-p Value | Effect | Tokens Considered |\n",
    "|-------------|--------|-------------------|\n",
    "| **0.1** | Very restrictive | Only top ~10% probability mass |\n",
    "| **0.5** | Moderate | Top ~50% probability mass |\n",
    "| **0.9** | Permissive | Top ~90% probability mass |\n",
    "| **1.0** | No filtering | All tokens considered |\n",
    "\n",
    "### Why Use Top-p?\n",
    "\n",
    "Top-p is **adaptive** ‚Äî it automatically adjusts how many tokens to consider based on the probability distribution. When the model is confident, fewer tokens are considered. When uncertain, more tokens are included.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "### Example: Low Top-p (0.2)\n",
    "\n",
    "With `top_p=0.2`, only tokens that make up the top 20% of probability mass are considered. This leads to **more predictable** output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low top-p = Restrictive, predictable output\n",
    "result = generator(\n",
    "    \"The cat sat on\",\n",
    "    max_new_tokens=10,\n",
    "    top_p=0.2,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Top-p 0.2 (Restrictive):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0k1l2m3",
   "metadata": {},
   "source": [
    "### Example: High Top-p (0.9)\n",
    "\n",
    "With `top_p=0.9`, tokens making up 90% of the probability mass are considered. This allows for **more diverse** completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc24e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High top-p = More diverse output\n",
    "result = generator(\n",
    "    \"The cat sat on\",\n",
    "    max_new_tokens=10,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Top-p 0.9 (Diverse):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "### üí° Key Takeaway\n",
    "\n",
    "- **Low top-p (0.1-0.3)** ‚Üí Conservative, sticks to high-probability tokens\n",
    "- **High top-p (0.8-0.95)** ‚Üí More variety, includes less likely tokens\n",
    "- **Common default** ‚Üí 0.9 or 0.95\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2m3n4o5",
   "metadata": {},
   "source": [
    "## üî¢ Top-k\n",
    "\n",
    "**Top-k** is a simpler alternative to top-p. Instead of using probability thresholds, it simply **limits selection to the k most likely tokens**.\n",
    "\n",
    "### How Top-k Works\n",
    "\n",
    "1. Sort all possible next tokens by probability\n",
    "2. Keep only the top `k` tokens\n",
    "3. Sample from these k tokens\n",
    "\n",
    "| Top-k Value | Effect |\n",
    "|-------------|--------|\n",
    "| **1** | Greedy decoding (always pick the most likely) |\n",
    "| **5-10** | Very focused, limited variety |\n",
    "| **20-50** | Moderate variety |\n",
    "| **100+** | High variety |\n",
    "\n",
    "### Top-k vs Top-p\n",
    "\n",
    "| Aspect | Top-k | Top-p |\n",
    "|--------|-------|-------|\n",
    "| **Selection** | Fixed number of tokens | Dynamic based on probability |\n",
    "| **Adaptivity** | Not adaptive | Adapts to confidence |\n",
    "| **Simplicity** | Simpler to understand | More nuanced |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "### Example: Top-k = 1 (Greedy Decoding)\n",
    "\n",
    "With `top_k=1`, the model **always picks the single most likely token**. This is called greedy decoding and produces completely deterministic output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28d783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k = 1 (Greedy - always pick most likely)\n",
    "result = generator(\n",
    "    \"The cat sat on\",\n",
    "    max_new_tokens=10,\n",
    "    top_k=1,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Top-k 1 (Greedy - Most Deterministic):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q7",
   "metadata": {},
   "source": [
    "### Example: Top-k = 50 (More Variety)\n",
    "\n",
    "With `top_k=50`, the model can choose from the **50 most likely tokens**, allowing for more diverse output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eed592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k = 50 (More variety)\n",
    "result = generator(\n",
    "    \"The cat sat on\",\n",
    "    max_new_tokens=10,\n",
    "    top_k=50,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Top-k 50 (More Variety):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8",
   "metadata": {},
   "source": [
    "### üí° Key Takeaway\n",
    "\n",
    "- **Top-k = 1** ‚Üí Greedy, completely deterministic\n",
    "- **Top-k = 10-50** ‚Üí Good balance for most tasks\n",
    "- **Top-k is simpler** but less adaptive than top-p\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s9",
   "metadata": {},
   "source": [
    "## üìè Output Length\n",
    "\n",
    "Controlling the **length of generated text** is crucial for practical applications. There are two main parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| **max_new_tokens** | Maximum number of NEW tokens to generate |\n",
    "| **max_length** | Maximum TOTAL length (input + output) |\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- `max_new_tokens` is generally preferred as it's more intuitive\n",
    "- If both are set, `max_new_tokens` takes precedence\n",
    "- Generation may stop early if the model produces an end-of-sequence token\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "### Example: Short Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short_output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate only 10 new tokens\n",
    "result = generator(\n",
    "    \"The cat sat on\",\n",
    "    max_new_tokens=10\n",
    ")\n",
    "print(\"Short output (10 tokens):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8s9t0u1",
   "metadata": {},
   "source": [
    "### Example: Longer Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a031429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate up to 100 new tokens\n",
    "result = generator(\n",
    "    \"The cat sat on\",\n",
    "    max_new_tokens=100\n",
    ")\n",
    "print(\"Longer output (100 tokens):\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9t0u1v2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Combining Parameters\n",
    "\n",
    "In practice, you'll often **combine multiple parameters** to achieve the desired output. Here are some common combinations:\n",
    "\n",
    "### Factual/Precise Output\n",
    "```python\n",
    "temperature=0.3, top_p=0.9, top_k=40\n",
    "```\n",
    "\n",
    "### Creative Writing\n",
    "```python\n",
    "temperature=0.9, top_p=0.95, top_k=100\n",
    "```\n",
    "\n",
    "### Code Generation\n",
    "```python\n",
    "temperature=0.2, top_p=0.9, top_k=20\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Balanced creative output\n",
    "result = generator(\n",
    "    \"Once upon a time in a magical forest,\",\n",
    "    max_new_tokens=50,\n",
    "    temperature=0.8,\n",
    "    top_p=0.92,\n",
    "    top_k=50,\n",
    "    do_sample=True\n",
    ")\n",
    "print(\"Balanced Creative Output:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0u1v2w3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Summary\n",
    "\n",
    "| Parameter | What It Controls | Low Value | High Value |\n",
    "|-----------|------------------|-----------|------------|\n",
    "| **Temperature** | Randomness | Deterministic, focused | Creative, diverse |\n",
    "| **Top-p** | Probability threshold | Conservative | Permissive |\n",
    "| **Top-k** | Number of candidates | Very focused | More variety |\n",
    "| **max_new_tokens** | Output length | Short responses | Long responses |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with defaults** and adjust based on results\n",
    "2. **Use temperature OR top-p**, not both at extreme values\n",
    "3. **Match parameters to your use case** (factual vs creative)\n",
    "4. **Experiment!** Different tasks benefit from different settings\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Further Reading\n",
    "\n",
    "- [Hugging Face Text Generation Documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
    "- [The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751) (Top-p paper)\n",
    "- [How to Generate Text with Transformers](https://huggingface.co/blog/how-to-generate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
