{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aI5xCxhgFT_b"
   },
   "source": [
    "# ðŸ¦¥ Fine-Tuning with Unsloth: Teaching Llama to Think Like DeepSeek-R1\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we'll fine-tune **Llama-3.2-3B-Instruct** using **Unsloth** on the [ServiceNow-AI/R1-Distill-SFT](https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT) dataset. The goal is to transfer DeepSeek-R1's **chain-of-thought reasoning capabilities** into the smaller Llama model.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **QLoRA** | Quantized Low-Rank Adaptation for memory-efficient fine-tuning |\n",
    "| **4-bit Quantization** | Running 3B models on consumer GPUs |\n",
    "| **LoRA Adapters** | Training only a small fraction of parameters |\n",
    "| **Knowledge Distillation** | Teaching smaller models to reason like larger ones |\n",
    "\n",
    "### Why Unsloth?\n",
    "\n",
    "Unsloth provides **2-5Ã— faster training** and **70% less memory usage** compared to standard Hugging Face training through custom CUDA kernels and optimized implementations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsKFqmcSHAog"
   },
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, install Unsloth and its dependencies. Unsloth automatically handles the installation of optimized CUDA kernels for your GPU architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gw8Qkx_R7HDt"
   },
   "source": [
    "## Step 2: Load the Quantized Model\n",
    "\n",
    "We load **Llama-3.2-3B-Instruct** with 4-bit quantization enabled. This reduces the model's memory footprint from ~12GB (FP16) to ~3GB, making it trainable on consumer GPUs.\n",
    "\n",
    "### Key Parameters Explained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "3986c8d4e8d94c63bd0064c3b6d39634",
      "44eea4d636e240fc8835744fe471dfb1",
      "234803ea32634784b9589cabd8817e60",
      "26b309629116488ea828d8d8f9e0e989",
      "6f2771608b414d6d98489b0f6f788846",
      "50c751aba4f94033b28e6a163e467d7d",
      "9bbd1be5140a408480bd9189dc04253a",
      "0aa2c3b4df384d139048a420485e45d2",
      "6f0766b0259741ffaa776043ea42dc19",
      "b63531a052a945c8b6c0a64ebf0d406c",
      "28063bbf203142a88f9d67253562c85a",
      "8a963768c831467c8fcc49d46c47d5ac",
      "cd38a15075f14353811585c059d2adda",
      "c22db0e5c92b4a22bc10d25ac535d17a",
      "874bb4c54a604f029f60a5982b96a6ef",
      "4299be0479234b9d86272d78c1b6d617",
      "dc0b6b903a884a279de6b6ac890fd031",
      "0e7f7245a303488c88fce9b3fa01a8bd",
      "b1a52b9f5812420a8127fa2600843aa3",
      "39f514fc7df94eeea0e8843305304dc8",
      "9705b1401fb044b1ace27c5328d9c0ca",
      "17a5635ccfdf401a946bb4ce3ae36426",
      "dc1ea522e79e4be8ab10adb1801798ab",
      "6582c3bd76894e09afaec13b194f56e7",
      "de6cad4060594bfcb869962e06bb8754",
      "841ea3a009f54a5c95411c3dfa01de48",
      "e4ed7c92292e491ab870f9783380ed0b",
      "20b2aecbd1a245799c910a03c0615997",
      "60c887a26eb449ccbe33b5c9fc0b0827",
      "e4d9e8f11117431881dbed484da701a2",
      "cb9988158c3e46e3901a5cf8ec6a1b59",
      "faad9ebabc624978a5f2043444c4ed94",
      "5c0fbd2f004348fe9189f969bf396384",
      "e7137fe56bfd4eab9bfd8d3ac0eee05a",
      "f0a00b27ae484dbb934eced87ab384b1",
      "aba94b0d12ed48c89e55946176d44fcd",
      "5043578d42bc467ca67b4bd7ecd328f5",
      "7885171484f249a7aea15e566c5799cc",
      "bb665ef5691d4d9785085476444f7995",
      "d364a98951f84e60880a5cf2b8ba6518",
      "cc8f05661d854db9a066fdb75557b9d7",
      "05cab1833bc54b199edaac591580020e",
      "a99bfe1d0cd44039bb2d65668c4e30d2",
      "c3035f551d464452bdd262ce43bfe983",
      "8573e8830a5b4fb7805dfc4488409a68",
      "5a49cfc9287f4a8ca4d2d3357163b2d0",
      "f5f5f221aed44662ba15029fdf0cd6fe",
      "21ccf4d5597a470eae02c7b1ee37bfce",
      "6fdfb76d98204322905716b3e2cc0ea6",
      "56e34fe1c84844bea3b4be810ede38ba",
      "357578b45c9f44e48aebd34aac887fd0",
      "0c89d8af5eb84bd6bf2816ff4a20387e",
      "6d67d6edb5904cf1ac5803bb78d8c289",
      "3a6937d266ed465197b925116c552ab4",
      "0f453cf38b9e4d4c9c541e3f8af28d50"
     ]
    },
    "id": "C1ge9m7ATkDJ",
    "outputId": "c445d5af-643c-4777-b2d8-6c6719cae9e1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! Unsloth also supports RoPE (Rotary Positinal Embedding) scaling internally.\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit, # Will load the 4Bit Quantized Model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Breakdown\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `model_name` | `\"unsloth/Llama-3.2-3B-Instruct\"` | Pre-trained model from Unsloth's optimized collection |\n",
    "| `max_seq_length` | `2048` | Maximum tokens per sequence (Unsloth handles RoPE scaling internally) |\n",
    "| `dtype` | `None` | Auto-detect optimal precision (FP16 for T4/V100, BF16 for Ampere+) |\n",
    "| `load_in_4bit` | `True` | Enable NF4 quantization for 8Ã— memory reduction |\n",
    "\n",
    "**Memory Impact**: Loading in 4-bit reduces VRAM from ~12GB to ~3GB, enabling training on GPUs with 8-16GB VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9SkpzVh72Qu"
   },
   "source": [
    "## Step 3: Attach LoRA Adapters (PEFT)\n",
    "\n",
    "Instead of fine-tuning all 3 billion parameters, we attach **LoRA (Low-Rank Adaptation)** layers to specific modules. This trains only ~0.1-1% of parameters while achieving comparable results to full fine-tuning.\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "LoRA decomposes weight updates into low-rank matrices:\n",
    "\n",
    "```\n",
    "W_new = W_original + (B Ã— A)\n",
    "```\n",
    "\n",
    "Where `A` and `B` are small matrices with rank `r`. Instead of updating the full weight matrix, we only train `A` and `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZGW3-3qTrQb",
    "outputId": "ee565266-ee0a-40f8-b473-fd82a917b832"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16, # a higher alpha value assigns more weight to the LoRA activations\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"w_formula.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration Explained\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `r` | `16` | **Rank** of low-rank matrices. Higher = more capacity but more memory. Common values: 8, 16, 32, 64 |\n",
    "| `target_modules` | `[\"q_proj\", \"k_proj\", ...]` | Transformer layers to apply LoRA. Targeting attention + MLP layers covers the most important weights |\n",
    "| `lora_alpha` | `16` | **Scaling factor** for LoRA updates. Higher alpha = stronger influence of fine-tuning |\n",
    "| `lora_dropout` | `0` | Dropout for regularization. Set to 0 for Unsloth's optimized kernels |\n",
    "| `bias` | `\"none\"` | Whether to train bias terms. \"none\" is fastest and often sufficient |\n",
    "| `use_gradient_checkpointing` | `\"unsloth\"` | Memory optimization for long sequences. Trades compute for memory |\n",
    "| `random_state` | `3407` | Seed for reproducibility |\n",
    "| `use_rslora` | `False` | Rank-Stabilized LoRA variant (experimental) |\n",
    "| `loftq_config` | `None` | Low-Rank Quantization config (disabled here) |\n",
    "\n",
    "### Target Modules\n",
    "\n",
    "```\n",
    "q_proj, k_proj, v_proj, o_proj  â†’ Attention mechanism\n",
    "gate_proj, up_proj, down_proj  â†’ MLP/Feed-Forward layers\n",
    "```\n",
    "\n",
    "These modules contain the majority of learnable parameters in transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9w3HK6Wudv7v"
   },
   "source": [
    "## Step 4: Load the Training Dataset\n",
    "\n",
    "We use the **R1-Distill-SFT** dataset which contains reasoning examples from DeepSeek-R1. Each example has:\n",
    "- `problem`: The input question\n",
    "- `reannotated_assistant_content`: The response with `<think>` reasoning tags\n",
    "\n",
    "This dataset teaches the model to \"think before answering\" like reasoning models (o1, DeepSeek-R1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "7c0a4b38d1d7402eb8cf77c77fce58b0",
      "9fe621cb24084223b59b2d5033b34525",
      "11c7dc67ce4b45dc835a8894ba16671b",
      "871a7651e1f44b079564fd9850db3682",
      "e46508dbafc24901a0376511d48e35f1",
      "dfcdc3c93a9b410da0a3d7b194d8a3a9",
      "429e02e020564457a09c4db5a96b060a",
      "4040d8ca2bb547cfb38e285c37acb375",
      "b5478ed7d8674b0cab44887988524b86",
      "7b65af0c59d441628ffde21bb2a6196d",
      "b5af080595ba47bdb7b657ff93ea2347",
      "49e48c8cd2264bf69f13021518f6fbcd",
      "f1ec7ba4353d40ee8a03b0caf26314b4",
      "7657f1cd02684cffbd83ab0842016ca2",
      "7f697da2beb742d597a000a3c46513da",
      "e1f3941da47541cc88987ade77342c45",
      "30761c259bbe488596bf1d7123b368ec",
      "31850a74ac6b4789b84d5ebb9b4a40fe",
      "c5ac13cdb03840189f681b74dca855fe",
      "2626520e7e6c416ead8a641b6a64797d",
      "7656298e537e4fe691d0edf3f0150d63",
      "e3d4c5b6a9934021b7b67da4ba077f7e",
      "045e5dcb1bbb4e379df3174611cd5d88",
      "ada7e8a40e5446ba9d5ba6415ce78ad1",
      "3705224018c84ace9f5f08512cd6ac11",
      "de420131d0874494aac41fd0246a3d6c",
      "1eb37e69246b44febec3532d0685df65",
      "16ef5d30de58486f85a65358d6ec27f0",
      "9b4137b89f134e26a908067d06b295e3",
      "22678ac9799449e59bb5bdc7fb56184f",
      "82b8ebe26d344499a5093b011765b828",
      "a4ec8b8c40d34e64a93fa4afc81fb80a",
      "9c2e010701824ef5800c7b39e6b19119",
      "2fc36246f6104bb3a862bd4f0fb9543e",
      "dac05618982f46c5b627077c93b6b472",
      "27172a9479f44b1cbb4773295e9cc9dd",
      "33307f836e544d09a7da44cfa4011261",
      "af7caaf833f34d6fa50a9cd77c291fc4",
      "4f7fa5000c854898a7e9c75d1e6a5a0d",
      "f728a63f1b42403380c4fca9dec0ef43",
      "2366d246304a4f3688168791675ae8e8",
      "6d09c76c5e2446349e87e513d19145af",
      "4b21fdd6ca33434994e0cf6c73c7b7db",
      "e44b502f1ac54fee84bda4efc418d566",
      "f3e229d630d34c479ff27100ec083ae0",
      "e8855f012b424d339b1671f04d1022af",
      "c9f803228b1f4dc3b10471dd46cf0a8b",
      "1679a55373744e3fb019be2b7aa5ece7",
      "1372072b472543cab6c021148f12cf42",
      "e2f50c8014344c3388b5c2046459ed2b",
      "97b2a36a72e84bdbb1f2a0017d948e7b",
      "e1161c8c62c04c18a7da169636645fda",
      "4ca914e2cee244bba1a3c3945c059bc8",
      "9472c7f60ae54a53ad8e3cf680b3679a",
      "75f5b87199e64d688fe713837846553e"
     ]
    },
    "id": "ooBI-FeKTu42",
    "outputId": "6fbc9682-f97f-42e3-f7c4-af39e9297f07"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\",'v0', split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrSE5oU5XOHc",
    "outputId": "6049d3bc-b755-49c4-b989-495f8213b8e8"
   },
   "outputs": [],
   "source": [
    "print(dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Structure\n",
    "\n",
    "The dataset contains ~170K examples with the following fields:\n",
    "- `problem`: The original question/task\n",
    "- `reannotated_assistant_content`: Response with `<think>` tags containing reasoning steps\n",
    "- `solution`: The final answer\n",
    "\n",
    "The `<think>` tags teach the model to **show its work** before answering, enabling chain-of-thought reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFALWqlE1DBS"
   },
   "source": [
    "## Step 5: Create the Prompt Template\n",
    "\n",
    "We define a prompt template that structures the input for training. The template includes:\n",
    "1. **System instruction**: Tells the model to reason step-by-step\n",
    "2. **Problem**: The actual question wrapped in `<problem>` tags\n",
    "3. **Thinking + Solution**: The reasoning and answer (filled during training)\n",
    "\n",
    "The `formatting_prompts_func` transforms each dataset example into this format and appends the EOS token to signal sequence end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e65f81ebaf24426d8e95b96d81467dfd",
      "75c0d8df22474c8695a0255ae110ace3",
      "4748c4edfc2b4c33ad5f6c9e5dc44538",
      "2d44e0f5e97747a39a40b7dc30e9009a",
      "7ec5089d705b4f23ad1155919ec67a20",
      "5e3ca722f07641e88a9a0874db690e9f",
      "f26ad8a17b764aa8bac33efb1cb6cad9",
      "90077c13ee9e46319e92eb77aebf346a",
      "468eb73c71ec4dbe84ed0e4bf69b6066",
      "fea2d44b4cd84655a5a13d1d09bf6c85",
      "c2c1ba0bd0a24819b834bc0812c35671"
     ]
    },
    "id": "Fx__TrpYTwGh",
    "outputId": "fb61bb27-efed-47a3-b736-bbf0c2f8748c"
   },
   "outputs": [],
   "source": [
    "r1_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
    "<problem>\n",
    "{}\n",
    "</problem>\n",
    "\n",
    "{}\n",
    "{}\n",
    "\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "  problems = examples[\"problem\"]\n",
    "  thoughts = examples[\"reannotated_assistant_content\"]\n",
    "  solutions = examples[\"solution\"]\n",
    "  texts = []\n",
    "\n",
    "  for problem, thought, solution in zip(problems, thoughts, solutions):\n",
    "    text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
    "    texts.append(text)\n",
    "\n",
    "  return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure the Trainer\n",
    "\n",
    "We use Hugging Face's **SFTTrainer** (Supervised Fine-Tuning Trainer) with carefully tuned hyperparameters:\n",
    "\n",
    "### SFTTrainer Configuration\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `dataset_text_field` | `\"text\"` | Column containing formatted prompts |\n",
    "| `max_seq_length` | `2048` | Truncate/pad sequences to this length |\n",
    "| `dataset_num_proc` | `2` | Parallel processes for data loading |\n",
    "| `packing` | `False` | Disable sequence packing (can enable for 5Ã— speedup with short sequences) |\n",
    "\n",
    "### Training Arguments Explained\n",
    "\n",
    "| Parameter | Value | Purpose |\n",
    "|-----------|-------|---------|\n",
    "| `per_device_train_batch_size` | `2` | Samples per GPU per step (limited by VRAM) |\n",
    "| `gradient_accumulation_steps` | `4` | Accumulate gradients over 4 steps â†’ effective batch size = 8 |\n",
    "| `warmup_steps` | `5` | Gradually increase learning rate to prevent early instability |\n",
    "| `max_steps` | `60` | Total training steps (increase for better results) |\n",
    "| `learning_rate` | `2e-4` | Standard for LoRA fine-tuning |\n",
    "| `fp16`/`bf16` | Auto | Use BF16 on Ampere+ GPUs, FP16 otherwise |\n",
    "| `optim` | `\"adamw_8bit\"` | 8-bit AdamW optimizer saves memory |\n",
    "| `weight_decay` | `0.01` | L2 regularization to prevent overfitting |\n",
    "| `lr_scheduler_type` | `\"linear\"` | Linear learning rate decay |\n",
    "\n",
    "### Effective Batch Size Calculation\n",
    "\n",
    "```\n",
    "Effective batch size = per_device_batch Ã— gradient_accumulation Ã— num_gpus\n",
    "                     = 2 Ã— 4 Ã— 1 = 8\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "2c2eb39680d544aba7bf7117560286da",
      "d246596186404c9188afb98d68673b16",
      "adf953630cb74b7ab62ae1f129681965",
      "c2dbab6270684188b047d2d4d3acafd1",
      "c0225ac27bd44d28b537ba69aba5a9fb",
      "45b22c65fcfb43b59ff6a65db8e56835",
      "5fadcfbe98c94949be1c8327e7922668",
      "d6bed43f902144f8bf9a3ca83f0839a1",
      "ef48ad961b834c898fa66d4df222a1eb",
      "87121a166c90442bb81ca3e10fedc49e",
      "2308ee8eee10437d9d4be3c0668e8484"
     ]
    },
    "id": "EyKsD0kGUgO4",
    "outputId": "1bcbd492-23ee-45f7-b2e9-54feb3b45782"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2, # Number of processors to use for processing the dataset\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # The batch size per GPU/TPU core\n",
    "        gradient_accumulation_steps = 4, # Number of steps to perform befor each gradient accumulation\n",
    "        warmup_steps = 5, # Few updates with low learning rate before actual training\n",
    "        max_steps = 60, # Specifies the total number of training steps (batches) to run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\", # Optimizer\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc for observability\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eTRJn1bWdGlS",
    "outputId": "6f65c78f-3cea-4739-a2d8-b73769c54fa8"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train the Model\n",
    "\n",
    "Run the training loop. With the current settings (60 steps), this takes approximately 5-15 minutes depending on your GPU.\n",
    "\n",
    "**Training Metrics to Watch**:\n",
    "- `loss`: Should decrease over time (target: < 1.0)\n",
    "- `grad_norm`: Should stay stable (spikes indicate instability)\n",
    "- `learning_rate`: Should decrease linearly after warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDVGMkUZiqdo"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
    "<problem>\n",
    "{}\n",
    "</problem>\n",
    "\"\"\"\n",
    "message = sys_prompt.format(\"How many 'r's are present in 'strawberry'?\")\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": message},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "response = tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test the Fine-Tuned Model\n",
    "\n",
    "Now let's test if the model learned to reason! We use the classic \"How many r's in strawberry?\" questionâ€”a task that trips up many LLMs.\n",
    "\n",
    "**Key changes for inference**:\n",
    "- `FastLanguageModel.for_inference(model)` enables 2Ã— faster inference\n",
    "- `temperature = 1.5` + `min_p = 0.1` for diverse, creative reasoning\n",
    "- `max_new_tokens = 1024` allows space for full chain-of-thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rkPtTZ6tT3h",
    "outputId": "03d788b7-cb27-43cb-dd3c-c4a409750082"
   },
   "outputs": [],
   "source": [
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Response\n",
    "\n",
    "The model should now show **explicit reasoning** in `<think>` tags before providing the answer. A successful fine-tune will count each 'r' systematically: s-t-**r**-a-w-b-e-**r**-**r**-y = 3 r's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "1664a53d85094f56a0c620b3b814770f",
      "f8a560cf1c2a479c9148c8447582e34d",
      "9e727dbfe1e24c6291d851400dd62f18",
      "03b4173616cd4789b7fe38a579dc021f",
      "e2ae0420818a47c59f0ea48fb16f8216",
      "b4f1a93b3ea045e2bfceff8d026179b9",
      "f3650c2639c5455ca8cab7a5fb7db0d5",
      "345a05f6bed0436dbe7648a8ee032dd8",
      "75be8dc2875447248c8ee2717a92941b",
      "55ff5f107e09406eaeb67289dc67d50e",
      "ce5ed7d7f16648f880c380ca0fa83ea4"
     ]
    },
    "id": "KgZe1TwWmxTV",
    "outputId": "18a68b14-a234-4b8f-d678-b18a36a992a3"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"chintan-001-3B\")  # Local saving\n",
    "tokenizer.save_pretrained(\"chintan-001-3B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Save the Model\n",
    "\n",
    "Save the fine-tuned model in two formats:\n",
    "\n",
    "### LoRA Adapters Only\n",
    "Saves only the trained LoRA weights (~50-100 MB). To use, you need the base model + these adapters.\n",
    "\n",
    "### GGUF Format (Optional)\n",
    "Converts to llama.cpp compatible format for use with Ollama, LM Studio, or llama.cpp directly. This creates a single portable file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vAoaMPSp834",
    "outputId": "582efe33-6963-4fe4-cc0e-4e1855a11acc"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"chintan-001-3B-GGUF\", tokenizer,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Deploy with Ollama (Optional)\n",
    "\n",
    "**Ollama** is a tool for running LLMs locally with a simple API. These cells:\n",
    "1. Install Ollama\n",
    "2. Start the Ollama server\n",
    "3. Register our GGUF model\n",
    "4. Allow interaction via `ollama run unsloth_model`\n",
    "\n",
    "After this, you can chat with your fine-tuned model from the terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X59Gv-ACrtOh",
    "outputId": "76e5ef56-3d4e-407f-b5de-83ced8eb4da2"
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3ha5JWkrqA8"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6L3bt-Zirro9",
    "outputId": "1eff3294-f6fc-47db-e039-ced22186669a"
   },
   "outputs": [],
   "source": [
    "print(tokenizer._ollama_modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BYCFwcMLr6tx",
    "outputId": "bdaf08cd-ea38-42a1-cad1-ec52fa7a3209"
   },
   "outputs": [],
   "source": [
    "!ollama create unsloth_model -f ./chintan-001-3B-GGUF/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Built\n",
    "We fine-tuned Llama-3.2-3B to think step-by-step like DeepSeek-R1, using only ~50 lines of code and consumer GPU hardware.\n",
    "\n",
    "### Techniques Used\n",
    "\n",
    "| Technique | Benefit |\n",
    "|-----------|---------|\n",
    "| **4-bit Quantization** | 8Ã— memory reduction (12GB â†’ 3GB) |\n",
    "| **LoRA Adapters** | Train 0.1% of parameters, comparable results |\n",
    "| **QLoRA** | Combine quantization + LoRA for extreme efficiency |\n",
    "| **Knowledge Distillation** | Transfer reasoning from larger models |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Increase `max_steps`** to 500-1000 for better quality\n",
    "2. **Experiment with `r`** (LoRA rank): 32 or 64 for more capacity\n",
    "3. **Try different datasets** for domain-specific fine-tuning\n",
    "4. **Upload to Hugging Face Hub** for sharing\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [R1-Distill Dataset](https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
